기계학습은 분류 와 회귀 로 나뉜다.
분류란 어느 클래스에 속하냐를 찾는거고, 회귀는 입력 데이터의 수치를 예측하는 것이다.
분류로 우리는 사진속 인물이 누구냐를 알수 있고, 회귀로 우리는 x가 3일때 y가 몇일지를 알수있다.

출력함수로, 항등함수(별거없이 그대로 y=a)와 소프트맥스 함수가 있는데, 소프트 맥스 함수의 출력은 0~1이다.
즉 a1,a2,a3등이 소프트맥스 함수를 거치면, 0.1,0.231,0.12.. 뭐 이런식이 된다면, 이걸 확률로도 계산이 가능하다.
그래서 이 확률값으로, 가장 높은 수를 찾아서 분류 문제를 해결할 수 있다.

그러나! 기계학습의 문제풀이 과정은 학습과 풀이로 나눌수 있는데, 학습이란 모델을 학습하는것이고, 풀이는 학습한 모델로
데이터를 추론하는 과정이다. 추론 단계에서는 소프트맥스 함수를 생략할 수 있는데(값의 크기는 변하지 않기 때문이다), 학습에서는
사용한다.

신경망에서 입력층은 말그대로 한번 추론에 필요한 입력하는 데이터 갯수다(예를들어, 28x28 픽셀의 그림을 넣을거면, 784개의 입력층이 필요),
출력층은 분류해야(혹은 예측해야)되는 범위다(책에서의 손글씨를 보고 숫자 예측은, 0~9까지 이기때문에, 10개의 출력층이 필요하다.)

전처리(pre-processing)이란, 신경망의 입력 데이터에 특정 변환을 가하는 작업이다!(normalization등).

배치(batch)란? 추론 결과랑 대응하는 이미지가 같이 저장되는 형식으로 하나로 묶은 입력 데이터를 배치라 한다.

# learning!
알고리즘을 처음부터 설계하는 대신, 이미지에서 특징을 추출하고 그 특징의 패턴을 기계학습으로 학습하는 방법.

간단한 학습은 세가지가 있는데, 데이터를 사람이 직접 구현한 알고리즘을 통해 결과를 도출하는 방법;
사람이 생각한 특징으로 기계학습을 시켜(KNN,SVM등) 결과를 도출하는 방법;
신경망(딥러닝)을 통해 사람의 통제없이 학습후 도출하는 방법이 있다.
마지막 딥러닝을 종단간 기계학습(end to end machine learning)이라고도 하는데, 처음부터 끝까지 기계가 한다는 뜻.
오버피팅 : 한 데이터 셋에만 지나치게 최적화된 상태로, 범용 능력을 갖추기 못한 상태.

손실함수(loss function) : 신경망 성능의 나쁨을 나타내는 지표로, 현재 신경망의 오차를 표현.
1)평균 제곱 오차(mean squared error, MSE): E = 1/2(SUM((Yk-tk)^2)) 이중 y는 신경망 출력, t는 정답.
원-핫 인코딩 : 한 원소만 1, 나머지 0으로 표현하는 표기법.
2)교차 엔트로피 오차(cross entropy error, CEE): E = -sum(tk*log(yk)), 즉 t가 원-핫 인코딩일때, CEE의 값은 정답일때의
출력이 전체값을 결정. 그중, 여러 데이터로 구한 전체 CEE의 합은 : E = -1/N*sum(sum(tnk*logynk)) n번째 데이터의 k차원째 값, N으로 나눠서 정규화, 즉 평균 손실 함수임

미니배치(mini-batch): 데이터가 몇만,몇백만이 넘을경우, 위와같이 손실함수를 다 구하는건 현실적이지 못하다, 그래서 그 중 랜덤하게 일정 량의 샘플만 뽑은것을
미니배치라 하는데, 이 미니배치에 대해서만 학습하는것을 미니배치 학습이라 한다.

# 신경망을 학습할 때, 정확도를 지표로 삼으면 매개변수의 미분이 쉽게 0 이 되기때문에, 손실함수를 지표로 삼는다.






